{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14153e2a-fb66-41c2-ace7-5e812401ec22",
   "metadata": {},
   "source": [
    "# Reddit API Scraper Tool\n",
    "---\n",
    "Code used to log into Reddit's API and scrape posts from subreddits.  In this project, the subreddits used were 'fountainpens' and pens'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35781e5-e688-424e-b2b2-fd099ecd9b3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Contents\n",
    "---\n",
    "- [Imports and Setup](#Imports-and-Setup)\n",
    "- [Functions](#Functions)\n",
    "- [Steps to Run](#Steps-to-Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4b729-829b-4e3f-8c41-5be45a5cc984",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Note :  I never got around to impleting datetime to automate this further.  Consider that a stretch goal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34e9208-2a8f-454d-bf30-2729588b783b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import getpass\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4612e0ce-0cd5-4f7e-8ae4-ae2b16b42512",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv('redditname')\n",
    "password = os.getenv('password')\n",
    "client_id = os.getenv('client_id')\n",
    "client_secret = os.getenv('client_secret')\n",
    "user_agent = os.getenv('user_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a701bf6e-121a-4f9b-8ec4-b8c0b9ae3c65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Authentication\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d77538e-fb37-4f75-8281-f17ffd2ad157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#create an informative header for your application - check if getting a response.\n",
    "headers = {'User-Agent': 'namehere/0.0.1'}\n",
    "\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth,\n",
    "    data=data,\n",
    "    headers=headers)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e4c795-8232-43fb-98b3-c11ba13ace37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'namehere/0.0.1'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve access token\n",
    "token = res.json()['access_token']\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babe3f93-4504-4aae-ad77-e384718135a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up heders and check if working\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbed7fa-c3b1-48bc-bda1-63f9ab91b640",
   "metadata": {},
   "source": [
    "### Functions\n",
    "---\n",
    "|Function|Purpose|\n",
    "|--------|-------|\n",
    "|scraper|Input subreddit to be scraped.  It will scrape 1000 posts in the subreddit and extract the post title and body of the post.  The function then generates a dataframe from this data, adds 'subreddit' column that adds a classifier string, drops duplicate rows, and outputs the dataframe.\n",
    "|csv_add|Input subreddit dataframes to be added to corpus.csv.  Function will add the dataframes, drop duplicates, reset the index, and re-write to corpus.csv with updated data.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94dd5845-2dbd-48f9-93b0-1365f51768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has been the most challenging piece of code for me in the course, I think partly because I'm not all all familiar with how using APIs work.\n",
    "# I spoke with Alanna, Hank, Hank AND Alanna, searched the internet, and ChatGPT\n",
    "\n",
    "def scraper(subreddit, classifier):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function accepts the following inputs:\n",
    "    subreddit: (str) Name of subreddit the user wants to scrape.\n",
    "    classifier: (str) classifier of subreddit:\n",
    "        fountainpens = 'fp'\n",
    "        pens = 'pens'\n",
    "    The function will scrape the Reddit subreddit for 100 posts.  It generates a JSON file\n",
    "    from the data and then extracts the title and selftext(body of post). The process loops\n",
    "    until it's collected 1000 posts.\n",
    "\n",
    "    A dataframe is generated using the title and selftext as columns.  A column is added \n",
    "    to the dataframe titled 'subreddit' that adds the classifier value to all cells.\n",
    "    Finally, any duplicates in the dataframe are dropped and the dataframe is returned!\n",
    "    \"\"\"\n",
    "\n",
    "    #Necessary variables for function to run\n",
    "    base_url = 'https://oauth.reddit.com/r/'\n",
    "    posts = []\n",
    "    limit = 100\n",
    "    after =  None\n",
    "    total_posts = 1000\n",
    "\n",
    "    #pulls data from Reddit API\n",
    "    while len(posts) <= total_posts:\n",
    "        scrapes = requests.get(base_url+subreddit,\n",
    "                               headers=headers,\n",
    "                               params= {'limit':limit,'after':after})\n",
    "        data = scrapes.json()\n",
    "        new_posts = data['data']['children']\n",
    "    \n",
    "        #appends data to posts list\n",
    "        for p in new_posts:\n",
    "            posts.append(p['data'])\n",
    "\n",
    "        #cycles to the next after string and starts again until 1000 posts are collected\n",
    "        after = data['data']['after']\n",
    "\n",
    "    # Get the selftext and title from each child\n",
    "    titles = [post['title'] for post in posts]\n",
    "    selftexts = [post['selftext'] for post in posts]\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame({'title': titles, 'selftext': selftexts})\n",
    "    df['subreddit'] = classifier\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    return(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f228c166-a37e-45db-9c92-9b9524815c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hank helped with showing me how to make this not write to csvs now that I have the data I need.\n",
    "def csv_add(subreddit1, subreddit2, export = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    subreddit1: (str) Name of first subreddit dataframe.\n",
    "    subreddit2: (str) Name of second subreddit dataframe.\n",
    "    This function readsd in the corpus.csv, concatenates the new data to it as 'subreddit1' and 'subreddit2'.\n",
    "    It then drops any duplicates, resets the data index and writes the updated dataframe badck to the\n",
    "    corpus.csv./\n",
    "    \"\"\"\n",
    "\n",
    "    #read in corpus.csv\n",
    "    corpus_df = pd.read_csv('./data/corpus.csv', index_col = 0)\n",
    "\n",
    "    #concatenates subreddit1 and subreddit2 to a temp dataframe\n",
    "    temp_df = pd.concat([corpus_df, subreddit1, subreddit2])\n",
    "\n",
    "    #drops any duplicates from the temp dataframe\n",
    "    temp_df.drop_duplicates(inplace = True)\n",
    "\n",
    "    #resets the index of the temp dataframe\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #rewrites the updated dataframe to the corpus.csv\n",
    "    if export:\n",
    "        temp_df.to_csv('./data/corpus_AFTER.csv')\n",
    "    return(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0201b1d-cf31-41ab-abe0-f675988abfe4",
   "metadata": {},
   "source": [
    "### Steps to Run\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a897640-0c68-4b2b-81fb-0abb3030cbf6",
   "metadata": {},
   "source": [
    "|Variable|Purpose|\n",
    "|--------|-------|\n",
    "|fountain_pens_df|Variable that holds 'fountainpens' subreddit data collected data from scraper function in a dataframe.|\n",
    "|pens_df|Variable that hold 'pens' subreddit data collected from scraper function in a dataframe.|\n",
    "|temp|Dataframe of data written to corpus.csv.  Returned from csv_add function to check number of fountainpens and pens subreddit post count.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2746a538-7cb5-4977-b722-0e819e5a31a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process the fountainpens subreddit\n",
    "fountain_pens_df = scraper('fountainpens', 'fp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58e21241-1944-4965-8ea3-587313787722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the pens subreddit\n",
    "pens_df = scraper('pens', 'pens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c53c25a-2291-47fd-b7b8-d12edf29d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranfer newly acquired dataframs to corpus.csv\n",
    "temp = csv_add(fountain_pens_df, pens_df, export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5286cb1d-7e99-4bf2-8f35-353cf7306d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "fp      1772\n",
       "pens    1278\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to see how many of each posts are collected\n",
    "temp['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260ef02-cf79-4599-b884-073324e7a0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
